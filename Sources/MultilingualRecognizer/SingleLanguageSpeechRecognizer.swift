import Foundation
@preconcurrency import Speech
@preconcurrency import AVFoundation

@available(macOS 26.0, *)
final class SingleLanguageSpeechRecognizer: @unchecked Sendable {
    private var inputSequence: AsyncStream<AnalyzerInput>?
    private var inputBuilder: AsyncStream<AnalyzerInput>.Continuation?

    private var transcriber: SpeechTranscriber?
    private var analyzer: SpeechAnalyzer?

    private var recognitionTask: Task<(), Never>?
    private var analysisTask: Task<(), Never>?  // Task for analyzeSequence()
    private var converter = BufferConverter()
    private let ui: UserInterface
    private let speechProcessor: SpeechProcessor
    private let locale: Locale
    private let localeIdentifier: String
    private let source: String

    var analyzerFormat: AVAudioFormat?

    init(ui: UserInterface, speechProcessor: SpeechProcessor, locale: String, source: String) {
        self.ui = ui
        self.speechProcessor = speechProcessor
        self.localeIdentifier = locale
        self.locale = Locale(identifier: locale)
        self.source = source
    }

    func setUpTranscriber() async throws {
        ui.status("üîß Setting up \(localeIdentifier) transcriber...")

        // Step 1: Create transcriber for the specified language
        // Note: Preset API may not be available in macOS 26.0, using manual configuration
        transcriber = SpeechTranscriber(
            locale: locale,
            transcriptionOptions: [],
            reportingOptions: [.volatileResults],
            attributeOptions: [.audioTimeRange]
        )

        guard let transcriber else {
            throw NSError(domain: "TranscriptionError", code: 1, userInfo: [NSLocalizedDescriptionKey: "Failed to setup \(localeIdentifier) transcriber"])
        }

        ui.status("‚úÖ \(localeIdentifier) transcriber created")

        // Step 2: Check if assets are available (optional but recommended)
        // We'll skip the download step for now as it may block

        // Step 3: Create input stream
        (inputSequence, inputBuilder) = AsyncStream<AnalyzerInput>.makeStream()

        guard let inputSequence else {
            throw NSError(domain: "TranscriptionError", code: 1, userInfo: [NSLocalizedDescriptionKey: "Failed to create input sequence"])
        }

        // Step 4: Create analyzer and get audio format
        // Use bestAvailableAudioFormat as recommended in docs
        let audioFormat = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [transcriber])
        guard let audioFormat else {
            throw NSError(domain: "TranscriptionError", code: 1, userInfo: [NSLocalizedDescriptionKey: "No compatible audio format available"])
        }
        self.analyzerFormat = audioFormat
        ui.status("üé§ Using audio format for \(localeIdentifier): \(audioFormat.description)")

        analyzer = SpeechAnalyzer(modules: [transcriber])
        ui.status("‚úÖ SpeechAnalyzer created for \(localeIdentifier)")

        // Step 7: Start reading results BEFORE starting analysis
        // This is critical per Apple's example
        let ui = self.ui
        let processor = self.speechProcessor
        let locale = self.localeIdentifier
        let source = self.source
        let localTranscriber = transcriber
        recognitionTask = Task { @Sendable in
            do {
                for try await case let result in localTranscriber.results {
                    let text = String(result.text.characters)
                    let startTime = CMTimeGetSeconds(result.range.start)
                    let duration = CMTimeGetSeconds(result.range.duration)
                    await processor.process(
                        text: text,
                        isFinal: result.isFinal,
                        startTime: startTime,
                        duration: duration,
                        alternativeCount: result.alternatives.count,
                        locale: locale,
                        source: source
                    )
                }
            } catch {
                ui.status("‚ùå \(locale) recognition failed: \(error)")
            }
        }

        // Step 6: Start analysis using analyzeSequence (non-blocking, structured concurrency)
        // We use a separate task so it doesn't block setup
        let localAnalyzer = analyzer
        analysisTask = Task { @Sendable in
            do {
                let lastSampleTime = try await localAnalyzer?.analyzeSequence(inputSequence)
                ui.status("üéØ \(locale) analysis completed, last sample: \(lastSampleTime?.seconds ?? 0)")
            } catch {
                ui.status("‚ùå \(locale) analysis failed: \(error)")
            }
        }

        // Give the analyzer a moment to actually start before returning
        try await Task.sleep(for: .milliseconds(50))
        ui.status("üéØ \(localeIdentifier) SpeechAnalyzer started successfully!")
    }

    func streamAudioToTranscriber(_ buffer: AVAudioPCMBuffer) async throws {
        guard let inputBuilder, let analyzerFormat else {
            throw NSError(domain: "TranscriptionError", code: 2, userInfo: [NSLocalizedDescriptionKey: "Invalid audio setup for \(localeIdentifier)"])
        }

        // Convert buffer to the correct format
        let converted = try self.converter.convertBuffer(buffer, to: analyzerFormat)
        let input = AnalyzerInput(buffer: converted)

        // Send to analyzer
        inputBuilder.yield(input)
    }

    func finishTranscribing() async throws {
        ui.status("üõë Finishing \(localeIdentifier) transcription...")

        // Stop input stream first
        inputBuilder?.finish()

        // Cancel tasks
        recognitionTask?.cancel()
        analysisTask?.cancel()

        // Give analyzer time to finish processing any pending input
        try await analyzer?.finalizeAndFinishThroughEndOfInput()

        // Wait for tasks to complete cancellation
        recognitionTask = nil
        analysisTask = nil

        // Add brief delay to ensure Speech framework resources are released
        try await Task.sleep(for: .milliseconds(100))

        // Clean up resources to prevent conflicts on restart
        analyzer = nil
        transcriber = nil
        inputBuilder = nil
        inputSequence = nil
        analyzerFormat = nil

        ui.status("‚úÖ \(localeIdentifier) transcription finished and resources cleaned up")
    }
}